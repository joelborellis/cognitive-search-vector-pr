{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search Vector Search Recall Measurement Code Sample\n",
    "\n",
    "This code demonstrates how to measure recall for vectors in Azure Cognitive Search using Azure Python SDK\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To run the code, install the following packages. Please use the latest pre-release version `pip install azure-search-documents --pre`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-search-documents --pre\n",
    "! pip install python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "import json\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.models import Vector  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,\n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SearchIndexerDataContainer,  \n",
    "    SearchIndexer,  \n",
    "    SearchIndexerDataSourceConnection,  \n",
    "    IndexingParameters,\n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    ")  \n",
    "from azure.storage.blob import BlobServiceClient \n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Configure environment variables  \n",
    "load_dotenv()  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  \n",
    "index_name = \"shadow-vector-index-blob\"  \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")  \n",
    "blob_connection_string = os.getenv(\"BLOB_CONNECTION_STRING\")  \n",
    "container_name = os.getenv(\"BLOB_CONTAINER_NAME\")\n",
    "index_blob_folder = os.getenv(\"INDEX_BLOB_FOLDER\")\n",
    "query_blob_name = os.getenv(\"QUERY_BLOB_NAME\")\n",
    "\n",
    "credential = AzureKeyCredential(key)\n",
    "search_field = \"contentVector\"\n",
    "\n",
    "# Supports cosine and euclidean\n",
    "metric = \"cosine\"\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shadow-query-blob-name'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_blob_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your search index\n",
    "\n",
    "Create your search index schema and vector search configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shadow-vector-index-blob created\n"
     ]
    }
   ],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"category\", type=SearchFieldDataType.String,\n",
    "                    filterable=True),\n",
    "    SearchField(name=\"titleVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"blob-vector-config\"),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"blob-vector-config\"),\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithm_configurations=[\n",
    "        HnswVectorSearchAlgorithmConfiguration(\n",
    "            name=\"blob-vector-config\",\n",
    "            kind=\"hnsw\",\n",
    "            parameters={\n",
    "                \"m\": 4,\n",
    "                \"efConstruction\": 400,\n",
    "                \"efSearch\": 500,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"blob-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL of the blob: https://shadowblobstorage.blob.core.windows.net/shadow-content/shadow-query-blob-name\n"
     ]
    }
   ],
   "source": [
    "# Connect to Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "blob_url = container_client.get_blob_client(query_blob_name).url\n",
    "print(f\"URL of the blob: {blob_url}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect your Blob Storage to a data source in Cognitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'shadow-vector-index-blob' created or updated\n"
     ]
    }
   ],
   "source": [
    "# Create a data source \n",
    "ds_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))\n",
    "container = SearchIndexerDataContainer(name=container_name, query=index_blob_folder)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=blob_connection_string,\n",
    "    container=container\n",
    ")\n",
    "data_source = ds_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an indexer\n",
    "\n",
    "Create or update an indexer to populate the search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shadow-vector-index-blob-indexer created\n"
     ]
    }
   ],
   "source": [
    "# Create an indexer  \n",
    "indexer_name = f\"{index_name}-indexer\"  \n",
    "parameters = IndexingParameters(configuration={\"parsingMode\": \"jsonArray\"})\n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index SciFact dataset\",  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name,\n",
    "    parameters=parameters\n",
    ")  \n",
    "  \n",
    "indexer_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "  \n",
    "# Run the indexer  \n",
    "indexer_client.run_indexer(indexer_name)  \n",
    "print(f' {indexer_name} created')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing indexing dataset from container\n",
    "Fetch train_vectors, id and map ids to it's list indices for calculating ground truth values and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of documents: 401603\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 27\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal no. of documents: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_data)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39m#with open(\"output.json\", \"w\") as f:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m  \u001b[39m#       json.dump(train_data, f)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m train_vectors, train_ids \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[(h[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontentVector\u001b[39m\u001b[39m\"\u001b[39m], h[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m train_data])\n\u001b[0;32m     29\u001b[0m train_id_to_indices \u001b[39m=\u001b[39m {}\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_ids)):\n",
      "Cell \u001b[1;32mIn[26], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal no. of documents: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_data)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39m#with open(\"output.json\", \"w\") as f:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m  \u001b[39m#       json.dump(train_data, f)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m train_vectors, train_ids \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[(h[\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mcontentVector\u001b[39;49m\u001b[39m\"\u001b[39;49m], h[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m train_data])\n\u001b[0;32m     29\u001b[0m train_id_to_indices \u001b[39m=\u001b[39m {}\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_ids)):\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "def load_train_data_from_container(container_client, folder_name):\n",
    "    blobs = container_client.list_blobs(name_starts_with=folder_name)\n",
    "\n",
    "    json_array = []\n",
    "    for blob in blobs:\n",
    "        blob_client = container_client.get_blob_client(blob)\n",
    "        blob_data = blob_client.download_blob().readall()\n",
    "        json_array.append(blob_data.decode('utf-8'))\n",
    "\n",
    "    return json_array\n",
    "\n",
    "# Load train data\n",
    "data = load_train_data_from_container(container_client, index_blob_folder)\n",
    "\n",
    "# Fetch vectors and id from train dataset and map ids to indices for ground truth and recall calculation\n",
    "train_data = []\n",
    "for d in data:\n",
    "    #print(json.dumps(d))\n",
    "    #train_data.extend(json.loads(d))\n",
    "    train_data.extend(d)\n",
    "print(f'Total no. of documents: {len(train_data)}')\n",
    "\n",
    "\n",
    "#with open(\"output.json\", \"w\") as f:\n",
    " #       json.dump(train_data, f)\n",
    "\n",
    "train_vectors, train_ids = zip(*[(h[\"contentVector\"], h[\"id\"]) for h in train_data])\n",
    "\n",
    "train_id_to_indices = {}\n",
    "for i in range(len(train_ids)):\n",
    "    train_id_to_indices[f'{train_ids[i]}'] = i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load querying dataset from container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch query vectors from query dataset\n",
    "test_data_contents = blob_service_client.get_blob_client(container=container_name, blob=query_blob_name).download_blob().readall()\n",
    "test_data = json.loads(test_data_contents)\n",
    "\n",
    "query_vectors = [h[search_field] for h in test_data]\n",
    "print(f'No. of query vectors: {len(query_vectors)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Ground Truth Values\n",
    "\n",
    "We calculate ground truth values for each query vector by finding the top 'k' neighbors in train_data based on their distance to the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metric distances between train and test vectors\n",
    "distances = cdist(query_vectors, train_vectors, metric=metric)\n",
    "\n",
    "# Get the indices of k closest neighbors for each test vector\n",
    "indices = np.argsort(distances, axis=1)[:, :k]\n",
    "\n",
    "# Create a 2D array of k closest neighbors for each test vector\n",
    "neighbors = np.take(train_ids, indices, axis=0)\n",
    "print(f'Top {k} neighbors id for 1st query vector: {neighbors[0]}')\n",
    "\n",
    "# Create a 2D array of the distances of k closest neighbors for each test vector\n",
    "distances = distances[np.arange(len(query_vectors))[:, np.newaxis], indices]\n",
    "print(f'Distance of top {k} neighbors for 1st query vector: {distances[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Vector search\n",
    "\n",
    "Perform vector search for all the query vectors and store the response ids for recall measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform vector search and fetch the ids from response for each query vector  \n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))  \n",
    "results = []  \n",
    "for query in test_data:  \n",
    "    vector = Vector(value=query[\"contentVector\"], k=k, fields=search_field)  \n",
    "    result = search_client.search(  \n",
    "        search_text=None,  \n",
    "        vectors=[vector],  \n",
    "        select=[\"id\"]  \n",
    "    )  \n",
    "    result_ids = [int(h['id']) for h in result]  \n",
    "    results.append(result_ids)  \n",
    "  \n",
    "print(f'Result ids for 1st query vector: {results[0]}')  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure distance between query vector and its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_query_response_distance(vector1, vector2, metric):\n",
    "    return float(cdist(vector1, vector2, metric=metric)[0])\n",
    "\n",
    "query_response_distances = np.empty((len(results), k))\n",
    "for i in range(len(results)):\n",
    "    for j in range(k):\n",
    "        if j >= len(results[i]):\n",
    "            query_response_distances[i][j] = float('inf')\n",
    "            continue\n",
    "        query_response_distances[i][j] = calculate_query_response_distance([query_vectors[i]], [train_vectors[train_id_to_indices[f\"{results[i][j]}\"]]], metric)\n",
    "\n",
    "print(f'Query Response distance for 1st query vector: {query_response_distances[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Recall\n",
    "\n",
    "Calculate a threshold value for each query which would be approximately the distance of the k'th vector for the respective query vector in the ground truth value. \n",
    "We count all those response vectors as relevant result which are below these threshold values and get the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold(data, count, epsilon):\n",
    "    return data[count - 1] + epsilon\n",
    "\n",
    "recall = np.zeros(len(results))\n",
    "\n",
    "for i in range(len(results)):\n",
    "    threshold = calculate_threshold(distances[i], k, 1e-3)\n",
    "    count = 0\n",
    "    for d in query_response_distances[i]:\n",
    "        if d <= threshold:\n",
    "            count += 1\n",
    "    recall[i] = count\n",
    "\n",
    "overall_recall = np.mean(recall) / float(k)\n",
    "print(f'Recall: {overall_recall}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
